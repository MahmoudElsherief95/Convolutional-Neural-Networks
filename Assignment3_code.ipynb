{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 3 - WS 2023 -->\n",
    "\n",
    "# Convolutional Neural Networks (22 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the third assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless it is explicitly requested!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with **Convolutional Neural Networks**. Essentially, a CNN is a multi-layer perceptron that uses convolutional instead of fully connected layers. Since convolutions are known to be useful for image processing, CNNs have become a powerful tool for learning features from images. However, they have proven to beat alternative architectures in a variety of other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nnumpy import Module\n",
    "from nnumpy.utils import sig2col\n",
    "from nnumpy.testing import gradient_check\n",
    "\n",
    "rng = np.random.default_rng(1856)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "The main difference of CNNs with the fully connected networks we tackled thus far, is the *convolution operation*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Math\n",
    "\n",
    "Mathematically, a (discrete) convolution operates on two functions, so that\n",
    "\n",
    "$$(f * g)[n] = \\sum_{k \\in \\mathbb{Z}} f[k] g[n - k].$$\n",
    "\n",
    "For image processing, the discrete functions $f$ and $g$ and replaced by images. After all, an image can be considered a function of pixel indices to pixel intensities. Also, images have (at least) two dimensions: width and height. Therefore, if we represent images as matrices of pixel intensities, we can write the convolution of an image $\\boldsymbol{X} \\in \\mathbb{R}^{H \\times W}$ with a so-called *kernel* $\\boldsymbol{K} \\in \\mathbb{R}^{R_1 \\times R_2}$ as follows:\n",
    "\n",
    "$$(\\boldsymbol{K} * \\boldsymbol{X})_{a,b} = \\sum_{i=1}^{R_1} \\sum_{j=1}^{R_2} k_{i,j} x_{a - i + 1,b - j + 1}.$$\n",
    "\n",
    "Instead of using the actual convolution operation, convolutional layers are often implemented as the *cross-correlation* of kernel and image instead:\n",
    "\n",
    "$$(\\boldsymbol{K} \\star \\boldsymbol{X})_{a,b} = \\sum_{i=1}^{R_1} \\sum_{j=1}^{R_2} k_{i,j} x_{a + i - 1,b + j - 1}.$$\n",
    "\n",
    "It might be useful to note that unlike the convolution, the cross-correlation is not commutative, i.e. $\\boldsymbol{K} \\star \\boldsymbol{X} \\neq \\boldsymbol{X} \\star \\boldsymbol{K}$, whereas $\\boldsymbol{K} * \\boldsymbol{X} = \\boldsymbol{X} * \\boldsymbol{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Cross-correlation vs Convolution (3 Points)\n",
    "\n",
    "Implementation-wise, there is little difference between cross-correlation and convolution. It is even quite straightforward to implement one, given an implementation of the other. To keep things simple, this exercise is limited to the one-dimensional variants of these operations (for now). How hard would it be to make your implementation of the convolution function commutative?\n",
    "\n",
    "> Implement functions to compute the cross-correlations and convolutions of one-dimensional signals. Obviously, you should **not** use functions like `np.convolve` or `np.correlate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e449e453030b5edd8cee54b27ee6ce97",
     "grade": false,
     "grade_id": "cell-8c80de2faae7eb03",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_correlation1d(x, k):\n",
    "    \"\"\"\n",
    "    Compute a one-dimensional cross-correlation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (L, ) ndarray\n",
    "        Input data for the cross-correlation.\n",
    "    k : (R, ) ndarray\n",
    "        Kernel weights for the cross-correlation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features : (L') ndarray\n",
    "        Cross-correlation of the input data with the kernel.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    arr = np.concatenate((k, np.zeros(x.shape[0] - k.shape[0])))\n",
    "\n",
    "    # lambda func for correlation\n",
    "    def cor(i): return np.sum(np.roll(arr, i).T * x)\n",
    "\n",
    "    features= np.fromfunction(np.vectorize(cor),(x.shape[0] - k.shape[0] + 1,),dtype='int')\n",
    "    return features\n",
    "\n",
    "    \n",
    "\n",
    "def convolution1d(x, k):\n",
    "    \"\"\"\n",
    "    Compute a one-dimensional convolution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (L, ) ndarray\n",
    "        Input data for the convolution.\n",
    "    k : (R, ) ndarray\n",
    "        Kernel weights for the convolution.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    features : (L', ) ndarray\n",
    "        Result of convolving the input data with the kernel.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    x1, x2 = (x, k) if x.shape[0] >= k.shape[0] else (k, x)\n",
    "\n",
    "    # Fill x2  with zeroes up to x1's length\n",
    "    arr = np.concatenate((np.flip(x2), np.zeros(x1.shape[0] - x2.shape[0])))\n",
    "\n",
    "    # lambda func for convolution\n",
    "    def con(i): return np.sum(np.roll(arr, i).T * x1)\n",
    "    features_ = np.fromfunction(np.vectorize(con),(x1.shape[0] - x2.shape[0] + 1,),dtype='int')\n",
    "    return features_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d899b1c6f1c9396d0f36c41f4538169",
     "grade": true,
     "grade_id": "cell-f4135c5eb6975af0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = rng.standard_normal(11)\n",
    "k = rng.standard_normal(3)\n",
    "corr = cross_correlation1d(x, k)\n",
    "assert isinstance(corr, np.ndarray), (\n",
    "    \"ex1: output of cross_correlation1d is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert corr.size == 9, (\n",
    "    \"ex1: output of cross_correlation1d has incorrect size (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e38e53c308f63c5af7dba231d9913d92",
     "grade": true,
     "grade_id": "cell-28acdeabbbbe0935",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1311d9c7f26facb4911b08f0984e69be",
     "grade": true,
     "grade_id": "cell-b2e2976b6cbc57a3",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = rng.standard_normal(11)\n",
    "k = rng.standard_normal(3)\n",
    "conv = convolution1d(x, k)\n",
    "assert isinstance(conv, np.ndarray), (\n",
    "    \"ex1: output of convolution1d is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert conv.size == 9, (\n",
    "    \"ex1: output of convolution1d has incorrect size (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbd985ea9699751e381b898c127d7474",
     "grade": true,
     "grade_id": "cell-f81d01bb5e648faa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Code\n",
    "\n",
    "This direct implementation does not offer a lot of features. For starters, it does not provide functionality to process multiple samples at once. Furthermore, practical implementations of convolutional layers normally require support for *channels*. After all, it is common practice to create multiple feature maps from a single signal to compensate for the spatial reduction through pooling and strides. These features can be incorporated in the mathematical formulation as follows:\n",
    "$$(\\boldsymbol{K} \\star \\boldsymbol{X})_{n,c_\\mathrm{out},a,b} = \\sum_{c_\\mathrm{in}=1}^{C_\\mathrm{in}} \\sum_{i=1}^{R_1} \\sum_{j=1}^{R_2} k_{c_\\mathrm{out},c_\\mathrm{in},i,j} x_{n,c_\\mathrm{in},a + i - 1,b + j - 1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this makes things a bit more complicated. It also introduces an extra loop over the number of input channels. In order to implement the above formula efficiently, we can use a trick that is commonly referred to as `im2col`. The idea of `im2col` is to represent the input tensor ($\\in \\mathbb{R}^{N \\times C_\\mathrm{in} \\times A \\times B}$) by a tensor in $\\mathbb{R}^{N \\times A' \\times B' \\times (C_\\mathrm{in} \\cdot R_1 \\cdot R_2)}$ where each \"column\" holds the elements in the window of the convolution. This allows the convolution to be computed as a simple matrix product with the (reshaped) kernel matrix $\\boldsymbol{K} \\in \\mathbb{R}^{C_\\mathrm{out} \\times (C_\\mathrm{in} \\cdot R_1 \\cdot R_2)}$, i.e.\n",
    "\n",
    "$$(\\boldsymbol{K} \\star \\boldsymbol{X})_{n,c_\\mathrm{out},a,b} = \\sum_{r=1}^{C_\\mathrm{in} \\cdot R_1 \\cdot R_2} x_{n,a,b,r} k_{r,c_\\mathrm{out}}.$$\n",
    "\n",
    "This trick is (efficiently) implemented in the `sig2col` function (slightly different name, since the function allows for modalities other than images). It takes **two inputs**: the signal to be convolved and the shape of the kernel as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sig2col on 1D signal\n",
    "x = np.arange(7)\n",
    "kernel_shape = (3, )\n",
    "sig2col(x, kernel_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image\n",
    "im = np.arange(16).reshape(4, 4)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  4,  5,  8,  9],\n",
       "       [ 1,  2,  5,  6,  9, 10],\n",
       "       [ 2,  3,  6,  7, 10, 11],\n",
       "       [ 4,  5,  8,  9, 12, 13],\n",
       "       [ 5,  6,  9, 10, 13, 14],\n",
       "       [ 6,  7, 10, 11, 14, 15]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3x2 windows in image as vectors\n",
    "kernel_shape = (3, 2)\n",
    "sig2col(im, (kernel_shape)).reshape(-1, 3 * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Multi-channel Convolutions (4 Points)\n",
    "\n",
    "Time to implement an actually practical convolution function that can handle multiple channels. Let us make it a 2D convolution at once.\n",
    "\n",
    " > Implement the `multi_channel_convolution2d` function below. You can use the `sig2col` function to implement the convolution by means of a dot product.\n",
    " \n",
    "**Hint:** When using the `sig2col` function, you might need to fiddle with the order of dimensions of your numpy arrays to align everything properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40f38c090e4d8b930dfc70e3a58db385",
     "grade": false,
     "grade_id": "cell-4c0f10089977d303",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_channel_convolution2d(x, k):\n",
    "    \"\"\"\n",
    "    Compute the multi-channel convolution of multiple samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, Ci, A, B)\n",
    "    k : (Co, Ci, R1, R2)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : (N, Co, A', B')\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    sig2col : can be used to convert (N, Ci, A, B) ndarray \n",
    "              to (N, Ci, A', B', R1, R2) ndarray.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    N, Ci, A, B = x.shape\n",
    "    Co, Ci2, R1, R2 = k.shape\n",
    "\n",
    "    assert Ci == Ci2\n",
    "\n",
    "    # x: (N, Ci, A, B) -> (N, Ci, A', B', R1, R2)\n",
    "    x = sig2col(x, (R1, R2))\n",
    "\n",
    "    N, Ci, A_dash, B_dash, A, B = x.shape\n",
    "\n",
    "    x = np.moveaxis(x, 1, 3)\n",
    "\n",
    "\n",
    "    x = x.reshape(N, A_dash, B_dash, Ci * R1 * R2)\n",
    "\n",
    "\n",
    "    k = k.reshape(Co, Ci * R1 * R2)\n",
    "\n",
    "\n",
    "    y = x @ k.T\n",
    "\n",
    "    # y: (N, A', B', Co) -> (N, Co, A', B')\n",
    "    y = np.moveaxis(y, 3, 1)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e33348e08284201aa6bcb2e1cf8e1ca6",
     "grade": true,
     "grade_id": "cell-c37b073450a2f668",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = rng.standard_normal(size=(10, 1, 28, 28))\n",
    "k = rng.standard_normal(size=(5, 1, 3, 3))\n",
    "s = multi_channel_convolution2d(x, k)\n",
    "assert isinstance(s, np.ndarray), (\n",
    "    \"ex2: output of multi_channel_convolution2d is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert s.shape == (x.shape[0], k.shape[0], 26, 26), (\n",
    "    \"ex2: output of multi_channel_convolution2d has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb2451009302e80a5286d43edf7fff98",
     "grade": true,
     "grade_id": "cell-8353455fb047cb09",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Module\n",
    "\n",
    "The multi-channel convolution pretty much covers the forward pass for a typical convolutional layer. For the backward pass, we will need the gradients of this operations. In the case of the simple convolution from the first exercise, it can easily be derived that the gradients w.r.t. inputs and weights are again convolutions, since\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_i} & = \\sum_a \\frac{\\partial L}{\\partial s_a} \\frac{\\partial s_a}{\\partial w_i} = \\sum_a \\delta_a x_{i+a} \\\\\n",
    "    \\frac{\\partial L}{\\partial x_i} & = \\sum_a \\frac{\\partial L}{\\partial s_a} \\frac{\\partial s_a}{\\partial x_i} = \\sum_{a'} w_{a'} \\delta_{i-a'},\n",
    "\\end{aligned}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial s_a}{\\partial w_i} & = \\frac{\\partial}{\\partial w_i} \\left( \\sum_r w_r x_{a+r} \\right) = x_{a+i} \\\\\n",
    "    \\frac{\\partial s_a}{\\partial x_i} & = \\frac{\\partial}{\\partial x_i} \\left( \\sum_r w_r x_{a+r} \\right) = w_{i - a}.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, this approach generalises to multi-channel convolutions. For the convolution of a 1D signal with $c_\\mathrm{i}$ channels so that the output has $c_\\mathrm{o}$ channels, it can be verified that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_{c_\\mathrm{o}, c_\\mathrm{i}, i}} & = \\sum_a \\frac{\\partial L}{\\partial s_{c_\\mathrm{o},a}} \\frac{\\partial s_{c_\\mathrm{o},a}}{\\partial w_{c_\\mathrm{o}, c_\\mathrm{i}, i}} = \\sum_a \\delta_{c_\\mathrm{o},a} x_{c_\\mathrm{i},i+a} \\\\\n",
    "    \\frac{\\partial L}{\\partial x_{c_\\mathrm{i}, i}} & = \\sum_{c_\\mathrm{o}} \\sum_a \\frac{\\partial L}{\\partial s_{c_\\mathrm{o},a}} \\frac{\\partial s_{c_\\mathrm{o},a}}{\\partial x_{c_\\mathrm{i}, i}} = \\sum_{c_\\mathrm{o}} \\sum_{a'} w_{c_\\mathrm{o}, c_\\mathrm{i}, a'} \\delta_{c_\\mathrm{o}, i-a'},\n",
    "\\end{aligned}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial s_{c_\\mathrm{o},a}}{\\partial w_{c_\\mathrm{o}, c, i}} & = \\frac{\\partial}{\\partial w_{c_\\mathrm{o}, c, i}} \\left( \\sum_{c_\\mathrm{i}} \\sum_r w_{c_\\mathrm{o}, c_\\mathrm{i}, r} x_{c_\\mathrm{i},a+r} \\right) = x_{c, a+i} \\\\\n",
    "    \\frac{\\partial s_{c_1,a}}{\\partial x_{c_2, i}} & = \\frac{\\partial}{\\partial x_{c_2,i}} \\left( \\sum_{c_\\mathrm{i}} \\sum_r w_{c_\\mathrm{o}, c_\\mathrm{i}, r} x_{c_\\mathrm{i}, a+r} \\right) = w_{c_1, c_2, i - a}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can conclude that the gradients of multi-channel convolutions can again be expressed as multi-channel convolutions - taking into account that we compute the convolutions for multiple samples at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Convolutional Layer (7 Points)\n",
    "\n",
    "Now, you should be able to implement both forward and backward pass in a module. Have you already thought about the shape of the bias parameter?\n",
    "\n",
    " > Implement the `Conv2D` module below. You can use the `multi_channel_convolution2d` function from the previous exercise to implement forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2411d4f8c2c7108c501b4e4fb0736080",
     "grade": false,
     "grade_id": "cell-18a7267c6de9b981",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conv2d(Module):\n",
    "    \"\"\" Numpy DL implementation of a 2D convolutional layer. \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # create parameters 'w' and 'b'\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        self.register_parameter('w', np.empty((out_channels, in_channels, *kernel_size)))\n",
    "\n",
    "        if use_bias:\n",
    "            self.register_parameter('b', np.empty(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self, seed: int = None):\n",
    "        \"\"\" \n",
    "        Reset the parameters to some random values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int, optional\n",
    "            Seed for random initialisation.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.w = rng.standard_normal(size=self.w.shape)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros_like(self.b)\n",
    "        \n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, Ci, H, W) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        feature_maps : (N, Co, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        out = multi_channel_convolution2d(x, self.w)\n",
    "\n",
    "        if self.use_bias:\n",
    "            out += self.b[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "\n",
    "        return out, x\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, Co, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, Ci, H, W) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        x = cache\n",
    "        R1, R2 = self.kernel_size\n",
    "\n",
    "        self.w.grad = np.moveaxis(\n",
    "            multi_channel_convolution2d(\n",
    "                np.moveaxis(x, 1, 0),\n",
    "                np.moveaxis(grads, 1, 0)\n",
    "            ),\n",
    "            # revert to keep same dimensions as x and grads had before\n",
    "            1, 0\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.b.grad = np.sum(grads, axis=(0, 2, 3))\n",
    "\n",
    "        return multi_channel_convolution2d(\n",
    "            # pad grads with zeroes as dimensions get reduced\n",
    "            np.pad(grads, ((0,), (0,), (R1 - 1,), (R2 - 1,))),\n",
    "            # switch axis to match dimensions\n",
    "            np.moveaxis(\n",
    "                # flip values on the last two axis\n",
    "                np.flip(self.w, (2, 3)),\n",
    "                1, 0\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f8a32c1b73e4ddf73c17a09c387460b",
     "grade": true,
     "grade_id": "cell-49d5bffe69b5f38d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "conv = Conv2d(3, 8, (5, 3))\n",
    "parameter_names = dict(conv.named_parameters())\n",
    "assert \"w\" in parameter_names, (\n",
    "    \"ex3: Conv2d module does not have 'w' parameter (-1 point)\"\n",
    ")\n",
    "assert \"b\" in parameter_names, (\n",
    "    \"ex3: Conv2d module does not have 'b' parameter (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "518b2baa08febe4165e4303c4d780c74",
     "grade": true,
     "grade_id": "cell-2dc8a656c40a0a4d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = rng.normal(size=(15, 3, 13, 13))\n",
    "s, cache = conv.compute_outputs(x)\n",
    "assert isinstance(s, np.ndarray), (\n",
    "    \"ex3: output of Conv2d.compute_outputs is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert s.shape == (len(x), conv.out_channels, 9, 11), (\n",
    "    \"ex3: output of Conv2d.compute_outputs has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13f2eb5df5eacd7dc4190e765333b954",
     "grade": true,
     "grade_id": "cell-7bb500eae7ef2b0a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "conv.zero_grad()\n",
    "g = conv.compute_grads(np.ones_like(s), cache)\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex3: output of Conv2d.compute_grads is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert g.shape == x.shape, (\n",
    "    \"ex3: output of Conv2d.compute_grads has incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25e8cf564a5b6a6932b1502c484c6e6a",
     "grade": true,
     "grade_id": "cell-c336f1ebc100598e",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert np.nonzero(conv.w.grad), (\n",
    "    \"ex3: Conv2d.compute_grads does not compute gradients for 'w' parameter (-0.5 points)\"\n",
    ")\n",
    "assert np.nonzero(conv.b.grad), (\n",
    "    \"ex3: Conv2d.compute_grads does not compute gradients for 'b' parameter (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfba64bd066b55a125388b16dccfb14f",
     "grade": true,
     "grade_id": "cell-9c971610ce9b4f9b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert gradient_check(conv, x, debug=True), (\n",
    "    \"ex3: Conv2d module does not pass gradient check (-4 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Although any activation function can be used in combination with convolutional neural networks, a very popular choice is the so-called *Rectified Linear Unit* (*ReLU*). The ReLU function maps all negative inputs to zero and all positive inputs to itself. Mathematically, this looks like\n",
    "\n",
    "$$\\mathrm{ReLU}(x) = \\begin{cases} 0 & x \\leq 0 \\\\ x & x > 0 \\end{cases}.$$\n",
    "\n",
    "An alternative activation function that is based on the ReLU, is the *Exponential Linear Unit* (*ELU*). Unlike the ReLU non-linearity, the ELU is able to keep the mean of the activations close to zero. It can be defined as follows:\n",
    "\n",
    "$$\\mathrm{ELU}(x \\mathbin{;} \\alpha) = \\begin{cases} \\alpha (e^x - 1) & x \\leq 0 \\\\ x & x > 0 \\end{cases}.$$\n",
    "\n",
    "The parameter $\\alpha$ in this non-linearity allows to specify the minimal negative value of the activations. Note that this $\\alpha$ is a hyper-parameter that must be fixed before training, and is thus not learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Some Linear Units (3 Points)\n",
    "\n",
    "A deep learning framework would not be complete without the ReLU and ELU activation functions. Time to add them!\n",
    "\n",
    " > Implement the `ReLU` and `ELU` activation function modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d60dcdbe9fb4d2444a024b2fe7af91ab",
     "grade": false,
     "grade_id": "cell-2bc64dc13483ab48",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\" NNumpy implementation of the Rectified Linear Unit. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        cache = s\n",
    "        a = np.maximum(s, 0)\n",
    "        return a , cache\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        s = cache\n",
    "        ds =np.where(s > 0, grads, 0)\n",
    "        \n",
    "        return ds\n",
    "\n",
    "\n",
    "class ELU(Module):\n",
    "    \"\"\" NNumpy implementation of the Exponential Linear Unit. \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.):\n",
    "        super().__init__()\n",
    "        if alpha < 0:\n",
    "            raise ValueError(\"negative values for alpha are not allowed\")\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        cache = s\n",
    "        a = np.where(s > 0, s, self.alpha * (np.exp(s) - 1))\n",
    "\n",
    "        return a, cache\n",
    "        \n",
    "    \n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        s = cache\n",
    "\n",
    "        ds = np.where(s > 0, grads, self.alpha * np.exp(s) * grads)\n",
    "        return ds \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fda7b6cf5aa09408903d74b5a357734",
     "grade": true,
     "grade_id": "cell-90a715ea66b6f60d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "s = np.linspace(-3, 3, 35).reshape(7, 5)\n",
    "phi = ReLU()\n",
    "a, cache = phi.compute_outputs(s)\n",
    "assert isinstance(a, np.ndarray), (\n",
    "    \"ex4: output of ReLU.compute_outputs is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert a.shape == s.shape, (\n",
    "    \"ex4: output of ReLU.compute_outputs has incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c7d299c909cd88f8f955ac16ad21a70",
     "grade": true,
     "grade_id": "cell-ad8556b201315749",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "g = phi.compute_grads(np.ones_like(s), cache)\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex4: output of ReLU.compute_grads is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert g.shape == s.shape, (\n",
    "    \"ex4: output of ReLU.compute_grads has incorrect shape (-1 point)\"\n",
    ")\n",
    "assert gradient_check(phi, x, debug=True), (\n",
    "    \"ex4: ReLU module does not pass gradient check (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8375230fc3f5b7d2860240956d66dca0",
     "grade": true,
     "grade_id": "cell-6f081fec120b8661",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "s = np.linspace(-3, 3, 35).reshape(7, 5)\n",
    "phi = ELU()\n",
    "a, cache = phi.compute_outputs(s)\n",
    "assert isinstance(a, np.ndarray), (\n",
    "    \"ex4: output of ELU.compute_outputs is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert a.shape == s.shape, (\n",
    "    \"ex4: output of ELU.compute_outputs has incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4116fd20143b044b33d86de1ee79123c",
     "grade": true,
     "grade_id": "cell-03c96045eb154ef0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "g = phi.compute_grads(np.ones_like(s), cache)\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex4: output of ELU.compute_grads is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert g.shape == s.shape, (\n",
    "    \"ex4: output of ELU.compute_grads has incorrect shape (-1 point)\"\n",
    ")\n",
    "assert gradient_check(phi, x, debug=True), (\n",
    "    \"ex4: ELU module does not pass gradient check (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Reduction\n",
    "\n",
    "The *weight sharing* in convolutional neural networks can drastically reduce the memory requirements for the weights. This effectively allows the input data to become larger, but since we need to store parts of the forward pass for back-propagation, the gains are rather limited. Of course, standard convolutions reduce the spatial dimensions, but this linear reduction is often too slow to counter the increased memory requirements due to network depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pooling\n",
    "\n",
    "In order to make working with big images feasible, we need techniques to reduce the spatial dimensions more strongly. This is where *pooling* layers prove very useful. A pooling layer reduces the spatial dimensions by combining a window of pixels to a single pixel. By sticking a pooling layer after every convolutional layer, the spatial dimensions are reduced exponentially, rather than linearly. This allows convolutional neural networks to process big chunks of data.\n",
    "\n",
    "There are different ways to summarise multiple pixels into a single pixel. Two very common pooling techniques are\n",
    "\n",
    " 1. **Average pooling** replaces the pixels by the mean intensity value in the window. \n",
    " 2. **Max pooling** replaces the pixels by the maximum intensity in the window.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Strides\n",
    "\n",
    "In modern convolutional neural networks, *strided* or *dilated* convolutions (see visualisations below) are often preferred over pooling. With strided convolutions, the windows are shifted The main advantage of strided or dilated convolutions over pooling is that they can be learnt. This means that instead of relying on a fixed pooling technique, it is possible to effectively learn how the pixels in the window are to be summarised. Also note that average pooling can indeed be represented as a strided convolution with weights $\\frac{1}{\\text{window size}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <figure style=\"display: inline-block; width: 49%;\">\n",
    "    <img style=\"padding: 46px 50px\" src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif\" />\n",
    "    <figcaption style=\"width: 100%;\"> Strided convolution </figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; width: 49%;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif\" />\n",
    "    <figcaption style=\"width: 100%; text-align: center;\"> Dilated convolution </figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "*visualisations taken from the [github repo](https://github.com/vdumoulin/conv_arithmetic) that comes with [this guide](https://arxiv.org/abs/1603.07285)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Pooling (5 Points)\n",
    "\n",
    "Since the `sig2col` function provides an array with the window elements in each column, it can also be used to implement pooling layers, when used correctly.\n",
    "\n",
    " > Implement the `MaxPool2d` module. You can use the `sig2col` function with its `stride` argument. You might also find the functions `np.take_along_axis` and `np.put_along_axis` useful.\n",
    " \n",
    "**Hint:** You can apply `sig2col` on `np.arange(x.size).reshape(x.shape)` to obtain the indices of the input after the `sig2col` operation. This could be useful for implementing the back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2dcaa67f35dee139c2544b1eef344f7",
     "grade": false,
     "grade_id": "cell-a61825c7b1912064",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaxPool2d(Module):\n",
    "    \"\"\" Numpy DL implementation of a max-pooling layer. \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = tuple(kernel_size)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, C, H, W) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, C, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "              \n",
    "        cache =  x\n",
    "        N, C, H, W = x.shape\n",
    "        H_a = int(H/self.kernel_size[0])\n",
    "        W_a = int(W/self.kernel_size[1]) \n",
    "        \n",
    "        x_column = sig2col(x, self.kernel_size, stride = self.kernel_size).reshape (N, C, H_a, W_a, -1 )\n",
    "        k_column = np.ones(self.kernel_size).reshape(-1)\n",
    "        avg = (self.kernel_size[0] * self.kernel_size [1])\n",
    "        \n",
    "        a = (x_column @ k_column.T) * 1/avg\n",
    "\n",
    "        return a, cache\n",
    "    \n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, C, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, C, H, W) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        self.zero_grad()\n",
    "        x = cache\n",
    "        N ,C ,H , W = x.shape\n",
    "        avg = self.kernel_size[0] * self.kernel_size[1]\n",
    "        \n",
    "        grads_ = grads.reshape(N, C, -1)\n",
    "        dx = np.repeat(grads_, avg ,axis= -1) * (1/avg)\n",
    "        dx = dx.reshape(N,C,H,W)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90c2aaaffff5c686d24b617e74f78c77",
     "grade": true,
     "grade_id": "cell-719ebf2001083942",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "pooling = MaxPool2d((2, 3))\n",
    "x = rng.standard_normal(size=(1, 1, 16, 18))\n",
    "p, cache = pooling.compute_outputs(x)\n",
    "assert isinstance(p, np.ndarray), (\n",
    "    \"ex5: output of MaxPool2d.compute_outputs is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert p.shape == x.shape[:2] + (8, 6), (\n",
    "    \"ex5: output of MaxPool2d.compute_outputs has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58b1850656456ae8a7b1662c2e065ff",
     "grade": true,
     "grade_id": "cell-8fe3422e86b80745",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ba46c91e9395b5dff09bc0f9d01f14b",
     "grade": true,
     "grade_id": "cell-d117d5535940aaa3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "g = pooling.compute_grads(np.ones_like(p), cache)\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex5: output of MaxPool2d.compute_grads is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert g.shape == x.shape, (\n",
    "    \"ex5: output of MaxPool2d.compute_grads has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c7e78785ce06d424daca640dca45e34",
     "grade": true,
     "grade_id": "cell-1fb2fd9fe50164fc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert gradient_check(pooling, x, debug=True), (\n",
    "    \"ex5: MaxPool2d module does not pass gradient check (-2 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for MaxPool2D: passed\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "pooling = MaxPool2d((2, 3))\n",
    "pool_check = gradient_check(pooling, rng.standard_normal(size=(1, 1, 16, 18)), debug=True)\n",
    "print(\"gradient check for MaxPool2D:\", \"passed\" if pool_check else \"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
